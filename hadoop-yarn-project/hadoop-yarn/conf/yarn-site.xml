<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
#
# AUTO-GENERATED, please try not to change it
# Time:  2021-12-14_05:52:53UTC
# Build: https://ltx1-jenkins.grid.linkedin.com:8443/job/hadoop-sre/job/grid-conf_batch-build/57/
# JIRA:  GRID-89384
#
-->
<configuration>
  <!-- Yarn Shuffle Configs -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle,spark_shuffle,spark_shuffle_311</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle_311.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle_311.classpath</name>
    <value>/export/apps/spark/shuffle/shuffle-311/*:/export/apps/spark/shuffle/shuffle-311/</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
    <value>org.apache.spark.network.yarn.YarnShuffleService</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services.spark_shuffle.classpath</name>
    <value>/export/apps/spark/shuffle/shuffle-230/*</value>
  </property>


  <!-- Spark Shuffle Server Config -->
  <property>
    <name>spark.authenticate</name>
    <value>true</value>
  </property>

  <property>
    <name>spark.shuffle.io.serverThreads</name>
    <value>80</value>
  </property>

  <property>
    <name>spark.shuffle.io.sendBuffer</name>
    <value>13107200</value>
  </property>

  <property>
    <name>spark.shuffle.io.receiveBuffer</name>
    <value>13107200</value>
  </property>

  <property>
    <name>spark.shuffle.server.chunkFetchHandlerThreads</name>
    <value>40</value>
  </property>

  <property>
    <name>spark.shuffle.service.index.cache.size</name>
    <value>800m</value>
  </property>

  <property>
    <name>spark.yarn.shuffle.stopOnFailure</name>
    <value>true</value>
  </property>
  <property>
    <name>spark.shuffle.server.pendingChunksPerAppMaxThreshold</name>
    <value>10000</value>
  </property>
  <property>
    <name>spark.shuffle.server.avgPendingChunkSizePerAppMinThreshold</name>
    <value>262144</value>
  </property>
  <property>
    <name>spark.shuffle.server.chunksTransferredPerSecPerAppMaxThreshold</name>
    <value>5000</value>
  </property>
  <property>
    <name>spark.shuffle.server.throttling.pendingChunksPerAppMaxThreshold</name>
    <value>10000</value>
  </property>
  <property>
    <name>spark.shuffle.server.throttling.avgChunkSizePerAppMinThreshold</name>
    <value>262144</value>
  </property>
  <property>
    <name>spark.shuffle.server.throttling.chunksTransferredPerSecPerAppMaxThreshold</name>
    <value>5000</value>
  </property>

  <property>
    <name>spark.shuffle.server.pendingChunksPerAppMaxThresholdSimulated</name>
    <value>10000</value>
  </property>
  <property>
    <name>spark.shuffle.server.avgPendingChunkSizePerAppMinThresholdSimulated</name>
    <value>1099511627776</value>
  </property>
  <property>
    <name>spark.shuffle.server.chunksTransferredPerSecPerAppMaxThresholdSimulated</name>
    <value>5000</value>
  </property>


  <property>
    <name>spark.shuffle.server.minChunkSizeInMergedShuffleFile</name>
    <value>2m</value>
  </property>

  <property>
    <name>spark.shuffle.io.connectionTimeout</name>
    <value>3600s</value>
  </property>
  <!-- Resource Manager Configs -->
  <property>
    <name>yarn.resourcemanager.scheduler.address</name>
    <value>ltx1-holdemrm01.grid.linkedin.com:8030</value>
  </property>
  <property>
    <name>yarn.resourcemanager.resource-tracker.address</name>
    <value>ltx1-holdemrm01.grid.linkedin.com:8031</value>
  </property>
  <property>
    <name>yarn.resourcemanager.address</name>
    <value>ltx1-holdemrm01.grid.linkedin.com:8032</value>
  </property>
  <property>
    <name>yarn.resourcemanager.admin.address</name>
    <value>ltx1-holdemrm01.grid.linkedin.com:8033</value>
  </property>

  <property>
    <name>yarn.resourcemanager.nodes.include-path</name>
    <value>/export/apps/hadoop/local/yarn.include</value>
  </property>
  <property>
    <name>yarn.resourcemanager.nodes.exclude-path</name>
    <value>/export/apps/hadoop/local/yarn.exclude</value>
  </property>
  <property>
    <name>yarn.resourcemanager.principal</name>
    <value>yarn/_HOST@GRID.LINKEDIN.COM</value>
  </property>
  <property>
    <name>yarn.resourcemanager.keytab</name>
    <value>/export/apps/hadoop/keytabs/${user.name}.keytab</value>
  </property>
  <property>
    <name>yarn.app.mapreduce.am.staging-dir</name>
    <value>/user</value>
  </property>
  <property>
    <name>yarn.resourcemanager.max-completed-applications</name>
    <value>5000</value>
  </property>

  <!-- HADOOP-10742 -->
  <property>
    <name>yarn.resourcemanager.scheduler.monitor.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.task.duration.killer.monitoring_interval_msecs</name>
    <value>300000</value>
  </property>

  <!-- GRID-1281 -->
  <property>
    <name>yarn.resourcemanager.work-preserving-recovery.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms</name>
    <value>60000</value>
  </property>

  <!-- Work preserve-->
  <property>
    <name>yarn.resourcemanager.recovery.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.resourcemanager.store.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.LeveldbRMStateStore</value>
  </property>
  <property>
    <name>yarn.resourcemanager.leveldb-state-store.path</name>
    <value>/export/content/holdem_resourcemanager_nfs/rm-state-store</value>
  </property>

  <property>
    <name>yarn.resourcemanager.monitor.capacity.elasticity.monitoring_interval</name>
    <value>180000</value>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.capacity.elasticity.elasticity_damping_factor</name>
    <value>.2</value>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.capacity.elasticity.total_elasticity_per_round</name>
    <value>.2</value>
  </property>

  <!-- GRID-39886 -->
  <property>
    <name>yarn.resourcemanager.nodemanagers.heartbeat-interval-ms</name>
    <value>1000</value>
  </property>

  <!-- LIHADOOP-50774 -->
  <property>
    <name>yarn.webapp.enable-rest-app-submissions</name>
    <value>false</value>
  </property>

  <!-- APA-18790. Increase maximum allowable AM attempts -->
  <property>
    <name>yarn.resourcemanager.am.global.max-attempts</name>
    <value>10</value>
  </property>

  <property>
    <name>yarn.resourcemanager.resource-tracker.client.thread-count</name>
    <value>80</value>
  </property>
  <property>
    <name>yarn.resourcemanager.node-ip-cache.expiry-interval-secs</name>
    <value>600</value>
  </property>
  <property>
    <name>yarn.resourcemanager.application.max-tags</name>
    <value>20</value>
  </property>
  <property>
    <name>yarn.resourcemanager.application.max-tag.length</name>
    <value>250</value>
  </property>
  <!-- Node Manager Configs -->
  <property>
    <name>yarn.nodemanager.localizer.address</name>
    <value>0.0.0.0:8040</value>
  </property>

  <property>
    <name>yarn.nodemanager.address</name>
    <value>0.0.0.0:8041</value>
  </property>

  <property>
    <name>yarn.nodemanager.admin-env</name>
    <value>MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX</value>
  </property>



  <property>
    <name>yarn.nodemanager.env-whitelist</name>
    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,HADOOP_YARN_HOME</value>
  </property>

  <property>
    <name>yarn.nodemanager.health-checker.script.path</name>
    <value>/export/apps/hadoop/site/bin/health_check.sh</value>
  </property>
  <property>
    <name>yarn.nodemanager.health-checker.interval-ms</name>
    <value>120000</value>
  </property>
  <property>
    <name>yarn.nodemanager.health-checker.script.timeout-ms</name>
    <value>60000</value>
  </property>
  <property>
    <description>Keytab for NM.</description>
    <name>yarn.nodemanager.keytab</name>
    <value>/export/apps/hadoop/keytabs/${user.name}.keytab</value>
  </property>
  <property>
    <description>The kerberos principal for the node manager.</description>
    <name>yarn.nodemanager.principal</name>
    <value>yarn/_HOST@GRID.LINKEDIN.COM</value>
  </property>

  <property>
    <name>yarn.nodemanager.container-executor.class</name>
    <value>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor</value>
  </property>

  <property>
    <name>yarn.nodemanager.recovery.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.recovery.dir</name>
    <value>/export/apps/hadoop/nm-state-store</value>
  </property>
  <property>
    <name>yarn.nodemanager.recovery.supervised</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.container-metrics.enable</name>
    <value>false</value>
    <description>GRID-10341 Flag to enable container metrics</description>
  </property>




  <property>
    <name>yarn.nodemanager.sleep-delay-before-sigkill.ms</name>
    <value>5000</value>
    <description>The time to wait between soft-killing a container (SIGTERM) and hard-killing it (SIGKILL).</description>
  </property>

  <!-- Resource Limits -->
  <property>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>1</value>
  </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>1024</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>75</value>
  </property>
  <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>249856</value>
  </property>

  <property>
    <name>yarn.nodemanager.container-executor.os.sched.priority.adjustment</name>
    <value>5</value>
  </property>

  <!-- NM Log Aggregation -->


  <property>
    <name>mapreduce.job.hdfs-servers</name>
    <value>hdfs://ltx1-holdemnn01.grid.linkedin.com:9000/,hdfs://ltx1-holdemnn02.grid.linkedin.com:9000/,hdfs://ltx1-holdemnn03.grid.linkedin.com:9000/,hdfs://ltx1-holdemnn04.grid.linkedin.com:9000/,hdfs://ltx1-holdemnn05.grid.linkedin.com:9000/</value>
  </property>

  <property>
    <name>yarn.log-aggregation-enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>hdfs://ltx1-holdemnn02.grid.linkedin.com:9000/system/app-logs</value>
  </property>
  <property>
    <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
    <value>yarn_aggregated_logs</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-seconds</name>
    <value>604800</value>
  </property>
  <property>
    <name>yarn.log.server.url</name>
    <value>http://ltx1-holdemjh01.grid.linkedin.com:19888/jobhistory/nmlogs</value>
  </property>
  <property>
    <name>yarn.log-aggregation.retain-check-interval-seconds</name>
    <value>21600</value> <!-- 6 hours / 4 times a day -->
  </property>

  <!-- HADOOP-12408 -->
  <property>
    <name>yarn.nodemanager.log-aggregation.compression-type</name>
    <value>gz</value>
  </property>
  <!-- Cgroup configurations -->
  <property>
    <name>yarn.nodemanager.linux-container-executor.resources-handler.class</name>
    <value>org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.mount</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.mount-path</name>
    <value>/sys/fs/cgroup</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.cgroups.hierarchy</name>
    <value>/hadoop.slice/yarn-containers</value>
  </property>
  <property>
    <name>yarn.nodemanager.linux-container-executor.group</name>
    <value>yarn</value>
  </property>
  <!-- WebAppProxy Configuration-->
  <property>
    <name>yarn.web-proxy.principal</name>
    <value>yarn/_HOST@GRID.LINKEDIN.COM</value>
  </property>
  <property>
    <name>yarn.web-proxy.keytab</name>
    <value>/export/apps/hadoop/keytabs/${user.name}.keytab</value>
  </property>
  <property>
    <name>yarn.web-proxy.address</name>
    <value>ltx1-holdemwp01.grid.linkedin.com:8080</value>
  </property>



  <!-- Applications' Configuration-->
  <property>
    <name>yarn.application.classpath</name>

    <value>$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/share/hadoop/common/*,$HADOOP_COMMON_HOME/share/hadoop/common/lib/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/*,$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,$HADOOP_YARN_HOME/share/hadoop/yarn/*,$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*,/export/apps/hadoop/site/lib/*</value>

  </property>

  <!-- GRID-3399 -->
  <property>
    <name>yarn.cluster.max-application-priority</name>
    <value>2</value>
  </property>
  <!-- HADOOP-11729 -->
  <property>
    <name>yarn.acl.enable</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.admin.acl</name>
    <value> sysadmin,griddev</value>
  </property>




  <!-- LIHADOOP-12541: Dynamic Capacity Scheduler -->
  <property>
    <name>yarn.scheduler.capacity.config.provider</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.DBBasedConfigurationProvider</value>
  </property>

  <property>
    <name>yarn.scheduler.capacity.config.path</name>
    <value>/export/content/holdem_resourcemanager_nfs/dcs</value>
  </property>

  <property>
    <name>yarn.scheduler.capacity.config.change.monitoring_interval_msecs</name>
    <value>120000</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.monitor.policies</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy

      ,org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ElasticityTunerPolicy

    </value>
  </property>

  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.monitoring_interval</name>
    <value>120000</value>
  </property>

  <property>
    <name>yarn.resourcemanager.monitor.capacity.elasticity.partitions</name>
    <value>highmem</value>
  </property>

  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.intra-queue-preemption.enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.intra-queue-preemption.preemption-order-policy</name>
    <value>priority_first</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.default.queues</name>
    <value>marathon,default,spark_default</value>
  </property>

  <property>
    <name>yarn.resourcemanager.monitor.capacity.preemption.max_wait_before_kill</name>
    <value>60000</value>
  </property>

  <property>
    <name>yarn.resourcemanager.scheduler.default.routing.queue</name>
    <value>public</value>
  </property>




  <!-- LIHADOOP-24160 OrgQueue v2 -->
  <property>
    <name>yarn.scheduler.configuration.store.class</name>
    <value>leveldb</value>
    <description>LIHADOOP-24160 Flag for enabling OrgQueue. Specifies using leveldb backend implementation for conf storage</description>
  </property>

  <property>
    <name>yarn.scheduler.configuration.mutation.acl-policy.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.QueueAdminConfigurationMutationACLPolicy</value>
  </property>

  <property>
    <name>yarn.scheduler.configuration.store.max-logs</name>
    <value>50</value>
  </property>

  <property>
    <name>yarn.scheduler.configuration.leveldb-store.path</name>
    <value>/export/content/holdem_resourcemanager_nfs/conf-store</value>
  </property>



  <!-- LIHADOOP-45336: Enable elasticity tuner for Faro partition -->
  <property>
    <name>yarn.resourcemanager.monitor.capacity.elasticity.partitions</name>
    <value>faro</value>
  </property>


  <property>
    <name>yarn.scheduler.queue-placement-rules</name>
    <value>com.linkedin.hadoop.yarn.server.resourcemanager.placement.QueueToQueuePlacementRule</value>
  </property>



  <!-- BEGIN HTTP/HTTPS for YARN -->
  <property>
    <name>yarn.http.policy</name>
    <value>HTTP_ONLY</value>
  </property>

  <property>
    <name>yarn.resourcemanager.webapp.address</name>
    <value>ltx1-holdemrm01.grid.linkedin.com:8088</value>
  </property>

  <property>
    <name>yarn.nodemanager.webapp.address</name>
    <value>0.0.0.0:8042</value>
  </property>
  <!-- Node label -->
  <property>
    <name>yarn.node-labels.enabled</name>
    <value>true</value>
  </property>
  <property>
    <name>yarn.node-labels.manager-class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager</value>
  </property>
  <property>
    <name>yarn.node-labels.fs-store.root-dir</name>
    <value>file:///export/content/holdem_resourcemanager_nfs/node-labels</value>
  </property>

</configuration>
